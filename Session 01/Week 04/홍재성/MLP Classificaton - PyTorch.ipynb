{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Set Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "fmnist_train = dset.FashionMNIST(\"./\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "fmnist_test = dset.FashionMNIST(\"./\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready for K-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Initiated\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform(m.weight)\n",
    "\n",
    "n_split = 5\n",
    "skf = StratifiedKFold(n_splits = n_split, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "# Test\n",
    "for fold_num, (train_idx, valid_idx) in enumerate(skf.split(np.arange(fmnist_train.__len__()), fmnist_train.targets)) : \n",
    "    print(f'Fold {fold_num} Initiated')\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_data = DataLoader(dataset=fmnist_train, batch_size = batch_size, sampler = train_subsampler)\n",
    "    valid_data = DataLoader(dataset=fmnist_train, batch_size = batch_size, sampler = valid_subsampler)\n",
    "    test_data  = DataLoader(dataset = fmnist_test, batch_size  = batch_size , shuffle = False)\n",
    "    \n",
    "\n",
    "    dataloaders = {}\n",
    "    dataloaders['train'] = train_data\n",
    "    dataloaders['valid'] = valid_data\n",
    "    dataloaders['test'] = test_data\n",
    "    # model.apply(reset_weights)\n",
    "\n",
    "    break;\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_Block(nn.Module) : \n",
    "    def __init__(self, input_dim, output_dim) : \n",
    "        super(FC_Block, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, output_dim, bias = True)\n",
    "        self.relu = nn.functional.relu\n",
    "        self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x) : \n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Classifier(nn.Module) : \n",
    "    def __init__(self) : \n",
    "        super(MLP_Classifier,self).__init__()\n",
    "        self.fc1 = FC_Block(28*28, 512)\n",
    "        self.fc2 = FC_Block(512,256)\n",
    "        self.fc3 = FC_Block(256,128)\n",
    "        self.fc4 = FC_Block(128, 64)\n",
    "        self.out_linear = nn.Linear(64,10)\n",
    "    \n",
    "    def forward(self, x) : \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.out_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "- https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs, early_stop) : \n",
    "    import time\n",
    "    import copy\n",
    "\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e+4\n",
    "    early_stop_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs) : \n",
    "        # print(f'Epoch {epoch}/{num_epochs -1}')\n",
    "        # print('=' * 10)\n",
    "\n",
    "        for phase in ['train','valid'] : \n",
    "            if phase == 'train' : \n",
    "                model.train()\n",
    "            elif phase == 'valid' : \n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0\n",
    "            running_corr = 0\n",
    "\n",
    "            for x,y in dataloaders[phase] : \n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase =='train') : \n",
    "                    output = model(x)\n",
    "                    loss = criterion(output, y)\n",
    "\n",
    "                    if phase == 'train' : \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item() * x.size(0)\n",
    "                running_corr += sum(torch.argmax(output, 1) == y)\n",
    "\n",
    "            if phase == 'train' : \n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corr / len(dataloaders[phase].dataset)\n",
    "\n",
    "            if phase == 'valid' and epoch_loss < best_loss : \n",
    "                print(f'On Epoch {epoch}, Best Model Saved with Valid Loss {round(epoch_loss, 4)} and Acc {round(epoch_acc.item(), 4)}')\n",
    "                \n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                early_stop_epoch = 0\n",
    "            elif phase == 'valid' : \n",
    "                early_stop_epoch += 1\n",
    "\n",
    "        if early_stop_epoch >= early_stop : \n",
    "            \"Early Stop Occured\"\n",
    "            break;\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training Complete in {time_elapsed//60}min {time_elapsed%60}sec')\n",
    "    print(f'Best Validation Loss : {best_loss} with Accuracy {best_acc}')\n",
    "\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(model, dataloaders) : \n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        for x,y in dataloaders['test'] : \n",
    "            x = x.view(-1,28*28).to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            prediction = model(x)\n",
    "            predictions.extend(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dataloaders(train_idx, valid_idx) : \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_data = DataLoader(dataset=fmnist_train, batch_size = batch_size, sampler = train_subsampler)\n",
    "    valid_data = DataLoader(dataset=fmnist_train, batch_size = batch_size, sampler = valid_subsampler)\n",
    "    test_data  = DataLoader(dataset = fmnist_test, batch_size  = batch_size , shuffle = False)\n",
    "    \n",
    "\n",
    "    dataloaders = {}\n",
    "    dataloaders['train'] = train_data\n",
    "    dataloaders['valid'] = valid_data\n",
    "    dataloaders['test'] = test_data\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Initiated\n",
      "==============================\n",
      "On Epoch 0, Best Model Saved with Valid Loss 0.0813 and Acc 0.1718\n",
      "On Epoch 1, Best Model Saved with Valid Loss 0.0724 and Acc 0.1741\n",
      "On Epoch 3, Best Model Saved with Valid Loss 0.0692 and Acc 0.1752\n",
      "On Epoch 4, Best Model Saved with Valid Loss 0.0679 and Acc 0.1763\n",
      "On Epoch 5, Best Model Saved with Valid Loss 0.0674 and Acc 0.1765\n",
      "On Epoch 6, Best Model Saved with Valid Loss 0.0625 and Acc 0.1778\n",
      "On Epoch 7, Best Model Saved with Valid Loss 0.0576 and Acc 0.1797\n",
      "On Epoch 8, Best Model Saved with Valid Loss 0.0571 and Acc 0.1798\n",
      "On Epoch 9, Best Model Saved with Valid Loss 0.0565 and Acc 0.1803\n",
      "On Epoch 12, Best Model Saved with Valid Loss 0.0562 and Acc 0.1805\n",
      "On Epoch 14, Best Model Saved with Valid Loss 0.0562 and Acc 0.1807\n",
      "On Epoch 15, Best Model Saved with Valid Loss 0.056 and Acc 0.1808\n",
      "On Epoch 16, Best Model Saved with Valid Loss 0.056 and Acc 0.1809\n",
      "Training Complete in 1.0min 45.41779446601868sec\n",
      "Best Validation Loss : 0.05603374361991882 with Accuracy 0.18088333308696747\n",
      "Fold 1 Initiated\n",
      "==============================\n",
      "On Epoch 0, Best Model Saved with Valid Loss 0.0926 and Acc 0.1686\n",
      "On Epoch 1, Best Model Saved with Valid Loss 0.0741 and Acc 0.1734\n",
      "On Epoch 2, Best Model Saved with Valid Loss 0.0709 and Acc 0.1742\n",
      "On Epoch 4, Best Model Saved with Valid Loss 0.0685 and Acc 0.1753\n",
      "On Epoch 7, Best Model Saved with Valid Loss 0.0581 and Acc 0.1787\n",
      "On Epoch 8, Best Model Saved with Valid Loss 0.0575 and Acc 0.1792\n",
      "On Epoch 10, Best Model Saved with Valid Loss 0.057 and Acc 0.1795\n",
      "On Epoch 11, Best Model Saved with Valid Loss 0.0567 and Acc 0.1798\n",
      "On Epoch 13, Best Model Saved with Valid Loss 0.0566 and Acc 0.1798\n",
      "On Epoch 14, Best Model Saved with Valid Loss 0.0561 and Acc 0.1798\n",
      "Training Complete in 1.0min 35.01071071624756sec\n",
      "Best Validation Loss : 0.0560671884059906 with Accuracy 0.17981666326522827\n",
      "Fold 2 Initiated\n",
      "==============================\n",
      "On Epoch 0, Best Model Saved with Valid Loss 0.0881 and Acc 0.169\n",
      "On Epoch 1, Best Model Saved with Valid Loss 0.0731 and Acc 0.1737\n",
      "On Epoch 2, Best Model Saved with Valid Loss 0.0716 and Acc 0.1737\n",
      "On Epoch 5, Best Model Saved with Valid Loss 0.0646 and Acc 0.1769\n",
      "On Epoch 6, Best Model Saved with Valid Loss 0.0632 and Acc 0.1772\n",
      "On Epoch 7, Best Model Saved with Valid Loss 0.0566 and Acc 0.1795\n",
      "On Epoch 8, Best Model Saved with Valid Loss 0.0555 and Acc 0.1797\n",
      "On Epoch 9, Best Model Saved with Valid Loss 0.0545 and Acc 0.18\n",
      "On Epoch 13, Best Model Saved with Valid Loss 0.054 and Acc 0.1803\n",
      "On Epoch 14, Best Model Saved with Valid Loss 0.0538 and Acc 0.1802\n",
      "On Epoch 15, Best Model Saved with Valid Loss 0.0537 and Acc 0.1804\n",
      "On Epoch 17, Best Model Saved with Valid Loss 0.0537 and Acc 0.1805\n",
      "On Epoch 22, Best Model Saved with Valid Loss 0.0537 and Acc 0.1804\n",
      "Training Complete in 2.0min 12.959803581237793sec\n",
      "Best Validation Loss : 0.053651882457733155 with Accuracy 0.18041667342185974\n",
      "Fold 3 Initiated\n",
      "==============================\n",
      "On Epoch 0, Best Model Saved with Valid Loss 0.0854 and Acc 0.1694\n",
      "On Epoch 1, Best Model Saved with Valid Loss 0.0758 and Acc 0.1729\n",
      "On Epoch 2, Best Model Saved with Valid Loss 0.0736 and Acc 0.1732\n",
      "On Epoch 4, Best Model Saved with Valid Loss 0.0636 and Acc 0.1773\n",
      "On Epoch 7, Best Model Saved with Valid Loss 0.0578 and Acc 0.1794\n",
      "On Epoch 8, Best Model Saved with Valid Loss 0.0575 and Acc 0.1792\n",
      "On Epoch 9, Best Model Saved with Valid Loss 0.0571 and Acc 0.1795\n",
      "On Epoch 10, Best Model Saved with Valid Loss 0.0566 and Acc 0.1798\n",
      "On Epoch 13, Best Model Saved with Valid Loss 0.0566 and Acc 0.1797\n",
      "On Epoch 14, Best Model Saved with Valid Loss 0.0558 and Acc 0.1801\n",
      "On Epoch 15, Best Model Saved with Valid Loss 0.0558 and Acc 0.1802\n",
      "On Epoch 17, Best Model Saved with Valid Loss 0.0558 and Acc 0.1803\n",
      "Training Complete in 1.0min 47.97656750679016sec\n",
      "Best Validation Loss : 0.05578199076652527 with Accuracy 0.18026666343212128\n",
      "Fold 4 Initiated\n",
      "==============================\n",
      "On Epoch 0, Best Model Saved with Valid Loss 0.0816 and Acc 0.1713\n",
      "On Epoch 1, Best Model Saved with Valid Loss 0.0783 and Acc 0.1713\n",
      "On Epoch 2, Best Model Saved with Valid Loss 0.0693 and Acc 0.1746\n",
      "On Epoch 3, Best Model Saved with Valid Loss 0.0653 and Acc 0.1765\n",
      "On Epoch 5, Best Model Saved with Valid Loss 0.0623 and Acc 0.1766\n",
      "On Epoch 7, Best Model Saved with Valid Loss 0.0576 and Acc 0.179\n",
      "On Epoch 8, Best Model Saved with Valid Loss 0.0573 and Acc 0.1793\n",
      "On Epoch 9, Best Model Saved with Valid Loss 0.0567 and Acc 0.1795\n",
      "On Epoch 10, Best Model Saved with Valid Loss 0.0566 and Acc 0.1797\n",
      "On Epoch 14, Best Model Saved with Valid Loss 0.0563 and Acc 0.1801\n",
      "On Epoch 15, Best Model Saved with Valid Loss 0.0561 and Acc 0.18\n",
      "On Epoch 18, Best Model Saved with Valid Loss 0.056 and Acc 0.1801\n",
      "On Epoch 20, Best Model Saved with Valid Loss 0.056 and Acc 0.1801\n",
      "On Epoch 25, Best Model Saved with Valid Loss 0.0559 and Acc 0.1801\n",
      "Training Complete in 2.0min 25.346558809280396sec\n",
      "Best Validation Loss : 0.05594448127746582 with Accuracy 0.18005000054836273\n"
     ]
    }
   ],
   "source": [
    "test_predictions = []\n",
    "\n",
    "n_split = 5\n",
    "skf = StratifiedKFold(n_splits = n_split, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "# Test\n",
    "for fold_num, (train_idx, valid_idx) in enumerate(skf.split(np.arange(fmnist_train.__len__()), fmnist_train.targets)) : \n",
    "    mlp = MLP_Classifier().to(device)\n",
    "    optimizer = optim.Adam(mlp.parameters(), lr = 0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    print(f'Fold {fold_num} Initiated')\n",
    "    print('='*30)\n",
    "    dataloaders = return_dataloaders(train_idx, valid_idx)\n",
    "    mlp_fitted = train_model(mlp, dataloaders, loss_fn, optimizer, lr_scheduler, 100, early_stop = 5)\n",
    "    test_prediction = predict_test(mlp_fitted, dataloaders)\n",
    "    test_predictions.append(test_prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy :  0.8972\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "for prediction in test_predictions : \n",
    "    test_preds.append(torch.cat([x for x in prediction], dim = 0).detach().cpu().numpy().reshape(-1,10))\n",
    "test_pred = np.argmax(np.mean(test_preds, axis = 0), axis = 1)\n",
    "test_accuracy = sum(test_pred == fmnist_test.targets.numpy()) / fmnist_test.__len__()\n",
    "print('Test Accuracy : ', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
