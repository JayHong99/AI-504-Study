{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Set Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "\n",
    "fmnist_train = dset.FashionMNIST(\"./\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
    "fmnist_test = dset.FashionMNIST(\"./\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ready for K-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Initiated\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def reset_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform(m.weight)\n",
    "\n",
    "n_split = 5\n",
    "skf = StratifiedKFold(n_splits = n_split, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "# Test\n",
    "for fold_num, (train_idx, valid_idx) in enumerate(skf.split(np.arange(fmnist_train.__len__()), fmnist_train.targets)) : \n",
    "    print(f'Fold {fold_num} Initiated')\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_data = DataLoader(dataset=fmnist_train, batch_size = batch_size, sampler = train_subsampler)\n",
    "    valid_data = DataLoader(dataset=fmnist_train, batch_size = batch_size, sampler = valid_subsampler)\n",
    "    test_data  = DataLoader(dataset = fmnist_test, batch_size  = batch_size , shuffle = False)\n",
    "    \n",
    "\n",
    "    dataloaders = {}\n",
    "    dataloaders['train'] = train_data\n",
    "    dataloaders['valid'] = valid_data\n",
    "    dataloaders['test'] = test_data\n",
    "    # model.apply(reset_weights)\n",
    "\n",
    "    break;\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_Block(nn.Module) : \n",
    "    def __init__(self, input_dim, output_dim) : \n",
    "        super(FC_Block, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, output_dim, bias = True)\n",
    "        self.relu = nn.functional.relu\n",
    "        self.batch_norm = nn.BatchNorm1d(output_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x) : \n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module) : \n",
    "    def __init__(self) : \n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            FC_Block(28*28,512),\n",
    "            FC_Block(512,256),\n",
    "            FC_Block(256,64),\n",
    "            FC_Block(64,16),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            FC_Block(16,64),\n",
    "            FC_Block(64,256),\n",
    "            FC_Block(256,512),\n",
    "            nn.Linear(512,28*28)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x) : \n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Classifier(nn.Module) : \n",
    "    def __init__(self) : \n",
    "        super(MLP_Classifier,self).__init__()\n",
    "        self.fc1 = FC_Block(28*28, 512)\n",
    "        self.fc2 = FC_Block(512,256)\n",
    "        self.fc3 = FC_Block(256,128)\n",
    "        self.fc4 = FC_Block(128, 64)\n",
    "        self.out_linear = nn.Linear(64,10)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.out_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_MLP(nn.Module) : \n",
    "    def __init__(self) : \n",
    "        super(AE_MLP, self).__init__()\n",
    "        self.AE = AutoEncoder()\n",
    "        self.MLP = MLP_Classifier()\n",
    "    def forward(self,x) : \n",
    "        x = x.view(-1,28*28)\n",
    "        x = self.AE(x)\n",
    "        x = self.MLP(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "- https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs, early_stop) : \n",
    "    import time\n",
    "    import copy\n",
    "\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e+4\n",
    "    early_stop_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs) : \n",
    "        # print(f'Epoch {epoch}/{num_epochs -1}')\n",
    "        # print('=' * 10)\n",
    "\n",
    "        for phase in ['train','valid'] : \n",
    "            if phase == 'train' : \n",
    "                model.train()\n",
    "            elif phase == 'valid' : \n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0\n",
    "            running_corr = 0\n",
    "\n",
    "            for x,y in dataloaders[phase] : \n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase =='train') : \n",
    "                    output = model(x)\n",
    "                    loss = criterion(output, y)\n",
    "\n",
    "                    if phase == 'train' : \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                running_loss += loss.item() * x.size(0)\n",
    "                running_corr += sum(torch.argmax(output, 1) == y)\n",
    "\n",
    "            if phase == 'train' : \n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corr / len(dataloaders[phase].dataset)\n",
    "\n",
    "            if phase == 'valid' and epoch_loss < best_loss : \n",
    "                print(f'On Epoch {epoch}, Best Model Saved with Valid Loss {round(epoch_loss, 4)} and Acc {round(epoch_acc.item(), 4)}')\n",
    "                \n",
    "                best_loss = epoch_loss\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                early_stop_epoch = 0\n",
    "            elif phase == 'valid' : \n",
    "                early_stop_epoch += 1\n",
    "\n",
    "        if early_stop_epoch >= early_stop : \n",
    "            \"Early Stop Occured\"\n",
    "            break;\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training Complete in {time_elapsed//60}min {time_elapsed%60}sec')\n",
    "    print(f'Best Validation Loss : {best_loss} with Accuracy {best_acc}')\n",
    "\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(model, dataloaders) : \n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        for x,y in dataloaders['test'] : \n",
    "            x = x.view(-1,28*28).to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            prediction = model(x)\n",
    "            predictions.extend(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dataloaders(train_idx, valid_idx) : \n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    train_data = DataLoader(dataset=fmnist_train, batch_size = batch_size, sampler = train_subsampler)\n",
    "    valid_data = DataLoader(dataset=fmnist_train, batch_size = batch_size, sampler = valid_subsampler)\n",
    "    test_data  = DataLoader(dataset = fmnist_test, batch_size  = batch_size , shuffle = False)\n",
    "    \n",
    "\n",
    "    dataloaders = {}\n",
    "    dataloaders['train'] = train_data\n",
    "    dataloaders['valid'] = valid_data\n",
    "    dataloaders['test'] = test_data\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Initiated\n",
      "==============================\n",
      "On Epoch 0, Best Model Saved with Valid Loss 0.0962 and Acc 0.1665\n",
      "On Epoch 1, Best Model Saved with Valid Loss 0.0865 and Acc 0.1695\n",
      "On Epoch 3, Best Model Saved with Valid Loss 0.0858 and Acc 0.1696\n",
      "On Epoch 5, Best Model Saved with Valid Loss 0.0812 and Acc 0.1711\n",
      "On Epoch 6, Best Model Saved with Valid Loss 0.0739 and Acc 0.173\n",
      "On Epoch 7, Best Model Saved with Valid Loss 0.0649 and Acc 0.1768\n",
      "On Epoch 8, Best Model Saved with Valid Loss 0.0649 and Acc 0.1771\n",
      "On Epoch 9, Best Model Saved with Valid Loss 0.0636 and Acc 0.1778\n",
      "On Epoch 10, Best Model Saved with Valid Loss 0.0634 and Acc 0.1776\n",
      "On Epoch 11, Best Model Saved with Valid Loss 0.0627 and Acc 0.1778\n",
      "On Epoch 12, Best Model Saved with Valid Loss 0.0627 and Acc 0.1782\n",
      "On Epoch 14, Best Model Saved with Valid Loss 0.0619 and Acc 0.1783\n",
      "On Epoch 18, Best Model Saved with Valid Loss 0.0619 and Acc 0.1785\n",
      "On Epoch 19, Best Model Saved with Valid Loss 0.0617 and Acc 0.1784\n",
      "Training Complete in 2.0min 10.130306243896484sec\n",
      "Best Validation Loss : 0.06166524389584859 with Accuracy 0.1784166693687439\n",
      "Fold 1 Initiated\n",
      "==============================\n",
      "On Epoch 0, Best Model Saved with Valid Loss 0.0968 and Acc 0.1675\n",
      "On Epoch 1, Best Model Saved with Valid Loss 0.085 and Acc 0.1697\n",
      "On Epoch 2, Best Model Saved with Valid Loss 0.0822 and Acc 0.1713\n",
      "On Epoch 3, Best Model Saved with Valid Loss 0.0814 and Acc 0.1726\n",
      "On Epoch 4, Best Model Saved with Valid Loss 0.073 and Acc 0.1743\n",
      "On Epoch 7, Best Model Saved with Valid Loss 0.0632 and Acc 0.1775\n",
      "On Epoch 8, Best Model Saved with Valid Loss 0.0627 and Acc 0.1778\n",
      "On Epoch 9, Best Model Saved with Valid Loss 0.0618 and Acc 0.1786\n",
      "On Epoch 13, Best Model Saved with Valid Loss 0.0616 and Acc 0.179\n",
      "On Epoch 14, Best Model Saved with Valid Loss 0.0607 and Acc 0.1793\n",
      "On Epoch 15, Best Model Saved with Valid Loss 0.0606 and Acc 0.1795\n",
      "On Epoch 19, Best Model Saved with Valid Loss 0.0606 and Acc 0.1795\n",
      "On Epoch 20, Best Model Saved with Valid Loss 0.0605 and Acc 0.1795\n",
      "On Epoch 21, Best Model Saved with Valid Loss 0.0605 and Acc 0.1793\n",
      "Training Complete in 2.0min 18.40065860748291sec\n",
      "Best Validation Loss : 0.06045583399136861 with Accuracy 0.179299995303154\n",
      "Fold 2 Initiated\n",
      "==============================\n",
      "On Epoch 0, Best Model Saved with Valid Loss 0.0933 and Acc 0.167\n",
      "On Epoch 1, Best Model Saved with Valid Loss 0.0848 and Acc 0.1695\n",
      "On Epoch 2, Best Model Saved with Valid Loss 0.078 and Acc 0.1722\n",
      "On Epoch 3, Best Model Saved with Valid Loss 0.0744 and Acc 0.1731\n",
      "On Epoch 4, Best Model Saved with Valid Loss 0.0718 and Acc 0.1754\n",
      "On Epoch 7, Best Model Saved with Valid Loss 0.0642 and Acc 0.1772\n",
      "On Epoch 8, Best Model Saved with Valid Loss 0.0633 and Acc 0.1776\n",
      "On Epoch 9, Best Model Saved with Valid Loss 0.0625 and Acc 0.178\n",
      "On Epoch 10, Best Model Saved with Valid Loss 0.0624 and Acc 0.1781\n",
      "On Epoch 11, Best Model Saved with Valid Loss 0.0618 and Acc 0.1784\n",
      "On Epoch 13, Best Model Saved with Valid Loss 0.0617 and Acc 0.1781\n",
      "On Epoch 14, Best Model Saved with Valid Loss 0.0609 and Acc 0.1786\n",
      "On Epoch 15, Best Model Saved with Valid Loss 0.0608 and Acc 0.1787\n",
      "On Epoch 16, Best Model Saved with Valid Loss 0.0607 and Acc 0.1788\n",
      "On Epoch 19, Best Model Saved with Valid Loss 0.0607 and Acc 0.179\n",
      "On Epoch 23, Best Model Saved with Valid Loss 0.0606 and Acc 0.1789\n",
      "On Epoch 27, Best Model Saved with Valid Loss 0.0606 and Acc 0.1789\n",
      "On Epoch 32, Best Model Saved with Valid Loss 0.0606 and Acc 0.179\n",
      "Training Complete in 3.0min 14.069879531860352sec\n",
      "Best Validation Loss : 0.0605806783358256 with Accuracy 0.1790333390235901\n",
      "Fold 3 Initiated\n",
      "==============================\n",
      "On Epoch 0, Best Model Saved with Valid Loss 0.0939 and Acc 0.1665\n",
      "On Epoch 1, Best Model Saved with Valid Loss 0.0908 and Acc 0.1678\n",
      "On Epoch 2, Best Model Saved with Valid Loss 0.0831 and Acc 0.1704\n",
      "On Epoch 3, Best Model Saved with Valid Loss 0.0798 and Acc 0.1723\n",
      "On Epoch 5, Best Model Saved with Valid Loss 0.0722 and Acc 0.1736\n",
      "On Epoch 7, Best Model Saved with Valid Loss 0.063 and Acc 0.1774\n",
      "On Epoch 8, Best Model Saved with Valid Loss 0.0621 and Acc 0.1781\n",
      "On Epoch 9, Best Model Saved with Valid Loss 0.0613 and Acc 0.178\n",
      "On Epoch 11, Best Model Saved with Valid Loss 0.0611 and Acc 0.1784\n",
      "On Epoch 12, Best Model Saved with Valid Loss 0.0607 and Acc 0.1784\n",
      "On Epoch 13, Best Model Saved with Valid Loss 0.0601 and Acc 0.1784\n",
      "On Epoch 14, Best Model Saved with Valid Loss 0.0598 and Acc 0.1791\n",
      "On Epoch 17, Best Model Saved with Valid Loss 0.0597 and Acc 0.1794\n",
      "On Epoch 18, Best Model Saved with Valid Loss 0.0597 and Acc 0.1794\n",
      "On Epoch 21, Best Model Saved with Valid Loss 0.0596 and Acc 0.1791\n",
      "On Epoch 22, Best Model Saved with Valid Loss 0.0595 and Acc 0.1789\n",
      "On Epoch 23, Best Model Saved with Valid Loss 0.0594 and Acc 0.1792\n",
      "On Epoch 24, Best Model Saved with Valid Loss 0.0594 and Acc 0.1791\n",
      "Training Complete in 2.0min 33.140443563461304sec\n",
      "Best Validation Loss : 0.0594099233229955 with Accuracy 0.17911666631698608\n",
      "Fold 4 Initiated\n",
      "==============================\n",
      "On Epoch 0, Best Model Saved with Valid Loss 0.1001 and Acc 0.1644\n",
      "On Epoch 1, Best Model Saved with Valid Loss 0.0952 and Acc 0.1668\n",
      "On Epoch 2, Best Model Saved with Valid Loss 0.0816 and Acc 0.1715\n",
      "On Epoch 4, Best Model Saved with Valid Loss 0.0743 and Acc 0.1735\n",
      "On Epoch 5, Best Model Saved with Valid Loss 0.0708 and Acc 0.175\n",
      "On Epoch 6, Best Model Saved with Valid Loss 0.0682 and Acc 0.1761\n",
      "On Epoch 7, Best Model Saved with Valid Loss 0.0632 and Acc 0.1779\n",
      "On Epoch 8, Best Model Saved with Valid Loss 0.0621 and Acc 0.1784\n",
      "On Epoch 9, Best Model Saved with Valid Loss 0.0621 and Acc 0.1783\n",
      "On Epoch 10, Best Model Saved with Valid Loss 0.0619 and Acc 0.1787\n",
      "On Epoch 11, Best Model Saved with Valid Loss 0.0616 and Acc 0.1785\n",
      "On Epoch 12, Best Model Saved with Valid Loss 0.0612 and Acc 0.1789\n",
      "On Epoch 13, Best Model Saved with Valid Loss 0.0608 and Acc 0.1795\n",
      "On Epoch 14, Best Model Saved with Valid Loss 0.0603 and Acc 0.1792\n",
      "On Epoch 15, Best Model Saved with Valid Loss 0.0602 and Acc 0.1793\n",
      "On Epoch 16, Best Model Saved with Valid Loss 0.0601 and Acc 0.1793\n",
      "On Epoch 17, Best Model Saved with Valid Loss 0.0601 and Acc 0.1793\n",
      "On Epoch 18, Best Model Saved with Valid Loss 0.06 and Acc 0.1793\n",
      "On Epoch 21, Best Model Saved with Valid Loss 0.0599 and Acc 0.1792\n",
      "Training Complete in 2.0min 18.046514987945557sec\n",
      "Best Validation Loss : 0.059917520236968995 with Accuracy 0.179216668009758\n"
     ]
    }
   ],
   "source": [
    "test_predictions = []\n",
    "\n",
    "n_split = 5\n",
    "skf = StratifiedKFold(n_splits = n_split, shuffle = True)\n",
    "\n",
    "# Test\n",
    "for fold_num, (train_idx, valid_idx) in enumerate(skf.split(np.arange(fmnist_train.__len__()), fmnist_train.targets)) : \n",
    "    ae_mlp = AE_MLP().to(device)\n",
    "    optimizer = optim.Adam(ae_mlp.parameters(), lr = 0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    print(f'Fold {fold_num} Initiated')\n",
    "    print('='*30)\n",
    "    dataloaders = return_dataloaders(train_idx, valid_idx)\n",
    "    ae_mlp = train_model(ae_mlp, dataloaders, loss_fn, optimizer, lr_scheduler, 100, early_stop = 5)\n",
    "    test_prediction = predict_test(ae_mlp, dataloaders)\n",
    "    test_predictions.append(test_prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy :  0.8928\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "for prediction in test_predictions : \n",
    "    test_preds.append(torch.cat([x for x in prediction], dim = 0).detach().cpu().numpy().reshape(-1,10))\n",
    "test_pred = np.argmax(np.mean(test_preds, axis = 0), axis = 1)\n",
    "test_accuracy = sum(test_pred == fmnist_test.targets.numpy()) / fmnist_test.__len__()\n",
    "print('Test Accuracy : ', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
